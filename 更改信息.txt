doudou: interesting!

git流程： 项目基础文件在A github上， 成员B去A的github上fork一份，之后会存放在自己的github中

从自己的github download下来项目文件，在自己这里改完后commit pull到自己的github

在在自己的github上 pull requests，真正上传到A的项目里，经过A审核后会更新。

最后，自己的github里要及时fetch upstream 同步更新代码

OK。

更改9.17：
1.向trainer.py中添加方法check_point，还有持续训练的函数。agent.get_model # 51行  添加policy_kwargs参数。
2.新增stable_baseline3的dqn算法，dqn还是存在test过程全部为-1的决策（ppo没有）。改为dqn模式，则Trainer.py 149行改为True
    注：因为DQN需要离散的action空间，所以在env中传入is_DQN参数，对对应部分进行分支结构处理
2.5. 在action_abstract（文件名不重要了）放入计算auc,roc的函数，输入为抽样次数（对样本的预测次数，多次预测以返回概率，这个地方有待商榷）
                                                        添加softmax后直接返回一个小数是否可以直接拿来算auc
3.删除env无用代码 method: return_terminal:
        gl_pct = self.account_information["total_assets"][-1] / self.initial_amount
        reward_pct = gl_pct

        # logger.record("environment/GainLoss_pct", (gl_pct - 1) * 100)
        # logger.record(
        #     "environment/total_assets",
        #     int(self.account_information["total_assets"][-1])
        # )
        #
        # logger.record("environment/total_reward_pct", (reward_pct - 1) * 100)
        # logger.record("environment/total_trades", self.sum_trades)
        # logger.record(
        #     "environment/avg_daily_trades",
        #     self.sum_trades / (self.current_step)
        # )
        # logger.record(
        #     "environment/avg_daily_trades_per_asset",
        #     self.sum_trades / (self.current_step) / len(self.assets)
        # )
        # logger.record("environment/completed_steps", self.current_step)
        # logger.record(
        #     "environment/sum_rewards", np.sum(self.account_information["reward"])
        # )
        # logger.record(
        #     "environment/retreat_proportion",
        #     self.account_information["total_assets"][-1] / self.max_total_assets
        # )

    # 多进程环境  此函数未被调用过
    # def get_multiproc_env(
    #     self, n: int = 10
    # ) -> Tuple[Any, Any]:
    #     def get_self():
    #         return deepcopy(self)
    #
    #     e = SubprocVecEnv([get_self for _ in range(n)], start_method="fork")
    #     obs = e.reset()
    #     return e, obs

    # 存动作信息
    # def save_action_memory(self) -> pd.DataFrame:
    #     if self.current_step == 0:
    #         return None
    #     else:
    #         return pd.DataFrame(
    #             {
    #                 "date": self.dates[-len(self.account_information["cash"]):],
    #                 "actions": self.actions_memory,
    #                 "transactions": self.transaction_memory
    #             }
    #         )

    # 存资产信息
    def save_asset_memory(self) -> pd.DataFrame:
        if self.current_step == 0:
            return None
        else:
            # 把目前的日期存到account_information中
            self.account_information["date"] = self.dates[-len(self.account_information["cash"]):]
            return pd.DataFrame(self.account_information)
    删除trainer.py无用代码：
        '''
    def get_data(self):
        """获取训练数据集和交易数据集"""
        train_data_path = os.path.join(self.data_dir, "train.csv")
        trade_data_path = os.path.join(self.data_dir, "trade.csv")
        if not (os.path.exists(train_data_path) or
                os.path.exists(trade_data_path)):
            print("数据不存在，开始下载")
            Data().pull_data()

        train_data = pd.read_csv(train_data_path)
        trade_data = pd.read_csv(trade_data_path)
        print("数据读取成功!")

        return train_data, trade_data
    '''

